# Gradient Descent 算法
1. Gradient-Descent 中的 cost function: 
   
        J(Θ) = 1/2m ∑ i=1~m (hθ(x(i)) - y(i))^2
   - m是数据集中数据点的个数 , 也就是样本数。
   - 1/2 是一个常量，这样是为了在求梯度的时候，二次方乘下来的2就和这里的1/2抵消了，自然就没有多余的常数系数，方便后续的计算，同时对结果不会有影响。
   - y 是数据集中每个点的真实y坐标的值 , 也就是类标签。
   - h 是我们的预测函数 (假设函数) 根据每一个输入x，根据Θ 计算得到预测的y值 , 即hΘ(x (i)) = Θ0 + Θ1x1(i)。
2. 对 cost-function 的每一个 theta 求偏导
    
        hΘ(x (i)) = Θ0 + Θ1x1(i)
        对 theta0 求偏导：1/m ∑ i=1~m (hθ(x(i)) - y(i))
        对 theta1 求偏导：1/m ∑ i=1~m (hθ(x(i)) - y(i)) * x1(i)

3. 为了代码编写的方便 , 将所有的公式保存为矩阵。
        
        J(Θ) = 1/2m (XΘ - 向量y)^T (XΘ - 向量y)
        ▽J(Θ) = 1/m X^T(XΘ - 向量y)